# ğŸ“˜ Linear Regression

## ğŸ”· 1. What is Linear Regression? 

**Linear Regression** is a **supervised statistical learning method** used to model the **linear relationship** between:

- **Dependent variable (Y)** â†’ Continuous  
- **Independent variable(s) (X)** â†’ One or more predictors  

### ğŸ¯ Goal
To find the **best-fitting straight line (or hyperplane)** that predicts **Y from X**.

### ğŸ“ Mathematical Form
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon
\]

Where:
- **Î²â‚€** â†’ Intercept  
- **Î²áµ¢** â†’ Regression coefficients  
- **Îµ** â†’ Random error term  

---

## ğŸ”· 2. Simple vs Multiple Linear Regression

### ğŸ“Œ Simple Linear Regression
\[
Y = \beta_0 + \beta_1 X
\]

**Used when:**
- Only one predictor  
- Easy interpretation  

---

### ğŸ“Œ Multiple Linear Regression
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \varepsilon
\]

**Used when:**
- Multiple influencing factors  
- Controls confounding variables  

---

## ğŸ”· 3. Mathematical Foundation

### ğŸ¯ Objective
Minimize the **Residual Sum of Squares (RSS)**:

\[
RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

Where:
- **yáµ¢** â†’ Actual value  
- **Å·áµ¢** â†’ Predicted value  

### ğŸ”¹ Ordinary Least Squares (OLS)
OLS estimates coefficients **Î²** such that the **sum of squared vertical distances** between observed and predicted values is minimized.

---

## ğŸ”· 4. Estimation of Coefficients (OLS Solution)

### ğŸ“ Matrix Form
\[
\hat{\beta} = (X^T X)^{-1} X^T Y
\]

### ğŸ’¡ Why this matters (Interview Point):
- Shows **linear algebra foundation**
- Explains **multicollinearity**
- If \( X^T X \) is not invertible â†’ coefficients become unstable  

---

## ğŸ”· 5. Assumptions of Linear Regression

Linear regression is valid **only if these assumptions hold**:

1ï¸âƒ£ **Linearity**  
Relationship between X and Y must be linear  

2ï¸âƒ£ **Independence**  
Observations must be independent  

3ï¸âƒ£ **Homoscedasticity**  
Constant variance of residuals  

4ï¸âƒ£ **Normality of Errors**  
Residuals should follow normal distribution  

5ï¸âƒ£ **No Multicollinearity**  
Predictors should not be highly correlated  

ğŸ“Œ **Violation of assumptions â†’ biased or unreliable results**

---

## ğŸ”· 6. Model Interpretation

### ğŸ”¹ Regression Coefficient (Î²)
- **Sign (+ / âˆ’)** â†’ Direction of relationship  
- **Magnitude** â†’ Strength of effect  

**Example:**
> Î²â‚ = 0.8 â†’ A 1-unit increase in NDVI increases mangrove density by **0.8 units**

---

### ğŸ”¹ Intercept (Î²â‚€)
- Expected value of Y when X = 0  
- Sometimes **not physically meaningful**, but required mathematically  

---

## ğŸ”· 7. Evaluation Metrics

### ğŸ“Œ RÂ² (Coefficient of Determination)
\[
R^2 = 1 - \frac{RSS}{TSS}
\]

- Measures **variance explained by the model**
- Ranges from **0 to 1**

---

### ğŸ“Œ Adjusted RÂ²
- Penalizes unnecessary predictors  
- Preferred for **multiple regression**

---

### ğŸ“Œ RMSE / MAE
- Measures **prediction error**
- Lower values indicate better performance  

---

## ğŸ”· 8. Residual Analysis (Model Diagnostics)

**Residuals:**
\[
\text{Residual} = y - \hat{y}
\]

### Diagnostic Checks:
- **Residual vs Fitted Plot** â†’ Linearity & homoscedasticity  
- **Q-Q Plot** â†’ Normality  
- **Durbinâ€“Watson Test** â†’ Autocorrelation  
- **VIF (Variance Inflation Factor)** â†’ Multicollinearity  

---

## ğŸ”· 11. Strengths

âœ” Simple and interpretable  
âœ” Strong statistical foundation  
âœ” Fast and computationally efficient  
âœ” Performs well when assumptions hold  

---

## ğŸ”· 12. Limitations

âŒ Cannot capture non-linear relationships  
âŒ Highly sensitive to outliers  
âŒ Multicollinearity destabilizes coefficients  
âŒ Assumptions often violated in real-world data  

---

