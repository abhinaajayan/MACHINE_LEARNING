# üå≤ Random Forest Classifier

## 1Ô∏è‚É£ What is a Random Forest?

A **Random Forest Classifier** is an **ensemble learning algorithm** that builds multiple decision trees and combines their predictions using **majority voting**.

### Formal Definition

\[
\hat{y} = \text{mode}\{h_1(x), h_2(x), ..., h_T(x)\}
\]

Where:
- \( h_t(x) \) = prediction of the *t-th decision tree*
- \( T \) = total number of trees

**Key idea:**  
> Many weak, uncorrelated trees together form a strong classifier.

---

## 2Ô∏è‚É£ Why Random Forest Was Needed 

### Problems with Decision Trees
- High variance
- Overfitting
- Sensitive to noise and small data changes

### Solution
Random Forest reduces variance using:
- **Bootstrap Aggregation (Bagging)**
- **Random Feature Selection**

---

## 3Ô∏è‚É£ Two Sources of Randomness

### AÔ∏è‚É£ Bootstrap Sampling (Bagging)
- Sampling **with replacement**
- Each tree sees a different dataset

If dataset size = **N**:
- ~63% unique samples
- ~37% left out ‚Üí **Out-Of-Bag (OOB) samples**

---

### BÔ∏è‚É£ Random Feature Subspace
At each split:
- Consider only **‚àöp features** (for classification)
- Instead of all **p features**

**Why?**
- De-correlates trees
- Prevents dominance of strong predictors
- Improves generalization

---

## 4Ô∏è‚É£ Algorithm ‚Äî Step by Step

1. Draw a bootstrap sample from training data
2. Train a decision tree:
   - No pruning
   - Random subset of features at each split
3. Repeat steps 1‚Äì2 for **T trees**
4. Prediction:
   - Each tree predicts a class
   - Final output = **majority vote**

---

## 5Ô∏è‚É£ Mathematical Intuition (Variance Reduction)

If variance of one tree = \( \sigma^2 \)

\[
Var_{RF} \approx \rho \sigma^2 + \frac{1 - \rho}{T} \sigma^2
\]

Where:
- \( \rho \) = correlation between trees
- \( T \) = number of trees

‚û°Ô∏è Lower correlation + more trees = lower variance

---

## 6Ô∏è‚É£ Splitting Criteria (Inside Each Tree)

Random Forest uses **CART (Classification and Regression Trees)**:
- **Gini Index** (default)
- **Entropy** (optional)

Each tree is grown **fully (unpruned)**.

---

## 7Ô∏è‚É£ Bias‚ÄìVariance Tradeoff

| Model | Bias | Variance |
|-----|-----|-----|
| Decision Tree | Low | High |
| Random Forest | Slightly higher | Much lower |

‚û°Ô∏è Random Forest trades **a small increase in bias** for a **large reduction in variance**.

---

## 8Ô∏è‚É£ Overfitting in Random Forest

- Rare compared to single decision trees
- Increasing trees generally **does not cause overfitting**

Overfitting may occur when:
- Data is extremely noisy
- Dataset is very small with very deep trees

---

## 9Ô∏è‚É£ Feature Importance

Random Forest provides built-in feature importance measures.

### AÔ∏è‚É£ Mean Decrease in Impurity (MDI)
\[
Importance = \sum \text{Gini Reduction}
\]

‚ö† Can be biased toward continuous features

---

### BÔ∏è‚É£ Permutation Importance (Preferred)
- Shuffle a feature
- Measure drop in model accuracy
- More reliable and model-agnostic

---

## üîü Out-Of-Bag (OOB) Error

- Uses leftover (~37%) samples
- Acts like internal cross-validation
- No separate validation set needed

```python
RandomForestClassifier(oob_score=True)

---

## 1Ô∏è‚É£1Ô∏è‚É£ Important Hyperparameters

| Parameter | Description |
|----------|------------|
| `n_estimators` | Number of trees in the forest |
| `max_depth` | Maximum depth of each tree (controls overfitting) |
| `max_features` | Number of features considered at each split |
| `min_samples_leaf` | Minimum samples required in a leaf node (smooths predictions) |
| `class_weight` | Handles class imbalance by assigning weights |

---

## 1Ô∏è‚É£2Ô∏è‚É£ Evaluation Metrics (Classification)

- **Accuracy** ‚Äì Overall correctness of the model  
- **Precision** ‚Äì Correct positive predictions out of all predicted positives  
- **Recall** ‚Äì Correct positive predictions out of all actual positives  
- **F1-Score** ‚Äì Harmonic mean of precision and recall  
- **ROC-AUC** ‚Äì Ability of the model to distinguish between classes  
- **Confusion Matrix** ‚Äì Summary of true vs predicted classifications  

---

## 1Ô∏è‚É£3Ô∏è‚É£ Strengths of Random Forest

‚úî Handles complex non-linear relationships  
‚úî Robust to noise and outliers  
‚úî No feature scaling or normalization required  
‚úî Works well with mixed data types (numerical + categorical)  
‚úî Excellent baseline model for classification tasks  

---

## 1Ô∏è‚É£4Ô∏è‚É£ Limitations of Random Forest

‚ùå Less interpretable compared to a single decision tree  
‚ùå High memory usage due to multiple trees  
‚ùå Slower prediction for very large forests  
‚ùå Poor extrapolation beyond the training data range  

---

