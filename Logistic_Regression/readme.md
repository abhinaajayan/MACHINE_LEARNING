# ğŸ“˜ Logistic Regression

## ğŸ”· 1. What is Logistic Regression?

**Logistic Regression** is a **supervised statistical learning method** used to model the relationship between:

- **Dependent variable (Y)** â†’ **Categorical (usually binary: 0 or 1)**
- **Independent variables (X)** â†’ One or more predictors (continuous or categorical)

### ğŸ¯ Goal
To estimate the **probability** that an observation belongs to a particular class.

ğŸ“Œ Despite its name, **logistic regression is a classification algorithm**, not a regression algorithm.

---

## ğŸ”· 2. Why Not Linear Regression for Classification?

Linear regression:
- Can predict values < 0 or > 1
- Assumes normally distributed errors
- Violates assumptions for binary outcomes

ğŸ”¹ Logistic regression solves this by:
- Constraining outputs between **0 and 1**
- Using a **sigmoid (logistic) function**
- Modeling **probabilities instead of raw values**

---

## ğŸ”· 3. Logistic Regression Model

### ğŸ“ Linear Combination
\[
z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
\]

### ğŸ“ Sigmoid (Logistic) Function
\[
P(Y=1 \mid X) = \frac{1}{1 + e^{-z}}
\]

This maps any real value to a **probability between 0 and 1**.

---

## ğŸ”· 4. Odds and Log-Odds (Very Important Concept)

### ğŸ“Œ Odds
\[
\text{Odds} = \frac{P(Y=1)}{P(Y=0)}
\]

### ğŸ“Œ Log-Odds (Logit)
\[
\log\left(\frac{P(Y=1)}{1 - P(Y=1)}\right)
= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\]

ğŸ“Œ Logistic regression models **log-odds as a linear function of predictors**.

---

## ğŸ”· 5. Interpretation of Coefficients

### ğŸ”¹ Coefficient (Î²áµ¢)
- Represents the **change in log-odds** for a one-unit increase in Xáµ¢
- Holding all other variables constant

### ğŸ”¹ Odds Ratio (OR)
\[
OR = e^{\beta_i}
\]

- **OR > 1** â†’ Increases probability of event
- **OR < 1** â†’ Decreases probability of event

**Example:**
> OR = 2 â†’ The odds of mangrove presence are **2 times higher** for a unit increase in NDVI.

---

## ğŸ”· 6. Estimation of Parameters

### ğŸ”¹ Maximum Likelihood Estimation (MLE)

Unlike linear regression (OLS), logistic regression uses **MLE**.

### ğŸ“ Likelihood Function
\[
L(\beta) = \prod_{i=1}^{n} P(y_i \mid x_i)
\]

### ğŸ“ Log-Likelihood
\[
\ell(\beta) = \sum_{i=1}^{n}
\left[ y_i \log(p_i) + (1 - y_i)\log(1 - p_i) \right]
\]

ğŸ¯ Objective: **Maximize log-likelihood**

---

## ğŸ”· 7. Loss Function

### ğŸ“Œ Binary Cross-Entropy (Log Loss)
\[
\text{Loss} = - \left[ y \log(p) + (1 - y)\log(1 - p) \right]
\]

- Penalizes confident but wrong predictions
- Convex â†’ guarantees global minimum

---

## ğŸ”· 8. Decision Boundary

- Default threshold: **0.5**
- If \( P(Y=1) \ge 0.5 \) â†’ Class 1
- If \( P(Y=1) < 0.5 \) â†’ Class 0

ğŸ“Œ Threshold can be adjusted for **imbalanced datasets**.

---

## ğŸ”· 9. Assumptions of Logistic Regression

1ï¸âƒ£ Binary dependent variable  
2ï¸âƒ£ Independent observations  
3ï¸âƒ£ Linear relationship between predictors and **log-odds**  
4ï¸âƒ£ No multicollinearity  
5ï¸âƒ£ Large sample size preferred  

ğŸ“Œ Logistic regression **does not assume normality or homoscedasticity**.

---

## ğŸ”· 10. Model Evaluation Metrics

### ğŸ“Š Classification Metrics
- Accuracy
- Precision
- Recall
- F1-score

### ğŸ“Š Probability Metrics
- ROC Curve
- AUC (Area Under Curve)
- Log Loss

### ğŸ“Š Confusion Matrix
| Actual / Predicted | 0 | 1 |
|-------------------|---|---|
| 0 | TN | FP |
| 1 | FN | TP |

---

## ğŸ”· 11. Regularization in Logistic Regression

| Method | Purpose |
|------|--------|
| L1 (Lasso) | Feature selection |
| L2 (Ridge) | Handles multicollinearity |
| Elastic Net | L1 + L2 |

---

## ğŸ”· 12. Strengths

âœ” Interpretable coefficients  
âœ” Probabilistic output  
âœ” Efficient and fast  
âœ” Strong statistical foundation  

---

## ğŸ”· 13. Limitations

âŒ Assumes linearity in log-odds  
âŒ Sensitive to outliers  
âŒ Cannot capture complex non-linear patterns  
âŒ Performance drops with highly overlapping classes  

---

