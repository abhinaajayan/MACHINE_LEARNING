# üå≥ Decision Tree 

## 1Ô∏è‚É£ What is a Decision Tree?

A **Decision Tree** is a **non-parametric, supervised learning algorithm** that learns a hierarchical set of decision rules to map input features (**X**) to an output variable (**y**).

**Formally:**
[ f : X \rightarrow y ]

**Key characteristics:**

* **Non-parametric** ‚Üí No assumption about data distribution
* **Rule-based** ‚Üí If‚ÄìElse conditions
* **Greedy algorithm** ‚Üí Chooses the best split at each step

---

## 2Ô∏è‚É£ Structure of a Decision Tree

| Component         | Meaning                              |
| ----------------- | ------------------------------------ |
| **Root Node**     | First split (most important feature) |
| **Internal Node** | Feature-based decision               |
| **Branch**        | Outcome of a condition               |
| **Leaf Node**     | Final prediction                     |

---

## 3Ô∏è‚É£ How a Decision Tree Learns

Decision Trees use **recursive binary partitioning**.

**Step-by-step learning:**

1. Start with all data at the root node
2. Try all features and all possible split points
3. Select the split that maximizes purity
4. Split the data
5. Repeat until a stopping condition is met

---

## 4Ô∏è‚É£ Splitting Criteria ‚Äî Mathematical Core

### AÔ∏è‚É£ Classification Trees

#### (a) Entropy

Measures impurity / uncertainty:

[ \text{Entropy} = - \sum p_i \log_2 p_i ]

* **0** ‚Üí Pure node
* **High value** ‚Üí Mixed classes

#### (b) Information Gain (IG)

[ IG = Entropy(parent) - \sum w_i \cdot Entropy(child_i) ]

‚û°Ô∏è Choose the split with **maximum Information Gain**

#### (c) Gini Index

[ Gini = 1 - \sum p_i^2 ]

* Faster than entropy
* Used by default in **scikit-learn**

---

### BÔ∏è‚É£ Regression Trees

Regression Trees minimize **prediction error**, not class impurity.

#### Mean Squared Error (MSE)

[ MSE = \frac{1}{n} \sum (y_i - \bar{y})^2 ]

* Split chosen to minimize **weighted MSE**
* Leaf node prediction = **mean value** of samples in that leaf

---

## 5Ô∏è‚É£ Why Decision Trees Can Model Non-Linearity

Decision Trees:

* Split feature space into **rectangular regions**
* Assign constant predictions within each region

**Mathematical form:**

[ f(x) = \sum c_m \cdot I(x \in R_m) ]

Where:

* ( R_m ) = region
* ( c_m ) = class label or mean value

‚û°Ô∏è Enables **piecewise non-linear approximation**

---

## 6Ô∏è‚É£ Decision Tree as a Weak / Strong Learner

* **Shallow tree** ‚Üí High bias, low variance
* **Deep tree** ‚Üí Low bias, high variance (overfitting)

### Bias‚ÄìVariance Tradeoff

| Tree Depth | Bias | Variance |
| ---------- | ---- | -------- |
| Shallow    | High | Low      |
| Deep       | Low  | High     |

---

## 7Ô∏è‚É£ Overfitting ‚Äî The Biggest Issue

### Why it happens

* Tree keeps splitting until leaves are pure
* Memorizes noise instead of learning patterns

### Control Methods (Pruning)

| Method                  | Meaning                   |
| ----------------------- | ------------------------- |
| `max_depth`             | Limit tree height         |
| `min_samples_split`     | Minimum samples to split  |
| `min_samples_leaf`      | Minimum samples in a leaf |
| Cost-complexity pruning | Penalizes tree depth      |

---

## 8Ô∏è‚É£ Optimization Problem

* Globally optimal decision tree learning is **NP-Hard**
* Uses **greedy local optimization**

‚û°Ô∏è Best split at each node **does not guarantee** the globally optimal tree

**Therefore:**

* Ensemble models outperform single trees

---

## 9Ô∏è‚É£ Decision Tree vs Linear Models

| Aspect              | Decision Tree | Linear Regression    |
| ------------------- | ------------- | -------------------- |
| Assumptions         | None          | Linearity, normality |
| Feature interaction | Automatic     | Manual               |
| Scaling             | Not needed    | Required             |
| Interpretability    | High          | Medium               |

---

## üîü Decision Tree in Feature Selection

Decision Trees perform **implicit feature selection**:

* Important features appear near the root
* Unimportant features may not appear

**Feature Importance:**

[ Importance = \sum (Impurity\ Reduction) ]

---

## 1Ô∏è‚É£1Ô∏è‚É£ Handling Different Data Types

‚úî Numerical features
‚úî Categorical features (encoded)
‚úî Missing values (via surrogate splits)

‚ùå No need for:

* Normalization
* Standardization

---

## 1Ô∏è‚É£2Ô∏è‚É£ Decision Tree in an ML Pipeline

```
Raw Data
   ‚Üì
Feature Engineering
   ‚Üì
Decision Tree Model
   ‚Üì
Prediction
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Evaluation Metrics

### Classification

* Accuracy
* Precision
* Recall
* F1-Score
* ROC‚ÄìAUC

### Regression

* RMSE
* MAE
* R¬≤

---


